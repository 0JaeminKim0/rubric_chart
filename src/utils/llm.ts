// LLM Score Engine - OpenAI Integration
import { Rubric, ScoreResponse } from '../types';

const SYSTEM_PROMPT = `You are a scoring engine.
You strictly apply the provided scoring definitions.
You do not explain or justify scores.
You output only integer values.`;

function buildUserPrompt(
  taskName: string,
  taskDescription: string | undefined,
  rubric: Rubric
): string {
  const context = taskDescription || "No additional context";
  
  return `Task:
${taskName}

Context:
${context}

Scoring Rubric:

Ease of Implementation (X):
1 = ${rubric.X["1"]}
2 = ${rubric.X["2"]}
3 = ${rubric.X["3"]}
4 = ${rubric.X["4"]}
5 = ${rubric.X["5"]}

Impact of Implementation (Y):
1 = ${rubric.Y["1"]}
2 = ${rubric.Y["2"]}
3 = ${rubric.Y["3"]}
4 = ${rubric.Y["4"]}
5 = ${rubric.Y["5"]}

Instructions:
- Assign exactly one integer score (1-5) for X and Y.
- Select the score whose definition best matches the task.
- Output JSON only. Example: {"X": 3, "Y": 4}`;
}

export interface LLMConfig {
  apiKey: string;
  baseUrl?: string;
  model?: string;
}

export async function scoreTask(
  config: LLMConfig,
  taskName: string,
  taskDescription: string | undefined,
  rubric: Rubric,
  retryCount: number = 0
): Promise<ScoreResponse> {
  const userPrompt = buildUserPrompt(taskName, taskDescription, rubric);
  
  // Use custom base URL if provided, otherwise default to OpenAI
  const baseUrl = config.baseUrl || 'https://api.openai.com/v1';
  const model = config.model || 'gpt-4o';
  const apiUrl = `${baseUrl}/chat/completions`;
  
  console.log('');
  console.log('='.repeat(60));
  console.log(`[LLM] üöÄ Starting LLM Score Request`);
  console.log('='.repeat(60));
  console.log(`[LLM] üìã Task Name: "${taskName}"`);
  console.log(`[LLM] üìù Description: "${taskDescription || 'None'}"`);
  console.log(`[LLM] ü§ñ Model: ${model}`);
  console.log(`[LLM] üåê API URL: ${apiUrl}`);
  console.log(`[LLM] üîë API Key: ${config.apiKey ? config.apiKey.substring(0, 10) + '...' : 'NOT SET'}`);
  console.log(`[LLM] üîÑ Retry Count: ${retryCount}`);
  console.log('-'.repeat(60));
  
  const requestBody = {
    model: model,
    messages: [
      { role: 'system', content: SYSTEM_PROMPT },
      { role: 'user', content: userPrompt }
    ],
    temperature: 0.1,
    max_tokens: 50,
    response_format: { type: 'json_object' }
  };
  
  console.log(`[LLM] üì§ Request Body (messages truncated):`);
  console.log(`[LLM]    - model: ${requestBody.model}`);
  console.log(`[LLM]    - temperature: ${requestBody.temperature}`);
  console.log(`[LLM]    - max_tokens: ${requestBody.max_tokens}`);
  
  try {
    const startTime = Date.now();
    
    console.log(`[LLM] ‚è≥ Sending request to OpenAI API...`);
    
    const response = await fetch(apiUrl, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${config.apiKey}`,
      },
      body: JSON.stringify(requestBody),
    });

    const elapsed = Date.now() - startTime;
    
    console.log(`[LLM] ‚è±Ô∏è  Response received in ${elapsed}ms`);
    console.log(`[LLM] üìä HTTP Status: ${response.status} ${response.statusText}`);

    if (!response.ok) {
      const errorText = await response.text();
      console.log(`[LLM] ‚ùå API Error Response:`);
      console.log(`[LLM]    ${errorText}`);
      throw new Error(`OpenAI API error: ${response.status} - ${errorText}`);
    }

    const data = await response.json() as {
      id?: string;
      model?: string;
      choices: Array<{
        message: {
          content: string;
        };
        finish_reason?: string;
      }>;
      usage?: {
        prompt_tokens: number;
        completion_tokens: number;
        total_tokens: number;
      };
    };
    
    console.log(`[LLM] ‚úÖ API Response Success!`);
    console.log(`[LLM] üì¶ Response ID: ${data.id || 'N/A'}`);
    console.log(`[LLM] ü§ñ Response Model: ${data.model || 'N/A'}`);
    
    if (data.usage) {
      console.log(`[LLM] üìà Token Usage:`);
      console.log(`[LLM]    - Prompt: ${data.usage.prompt_tokens}`);
      console.log(`[LLM]    - Completion: ${data.usage.completion_tokens}`);
      console.log(`[LLM]    - Total: ${data.usage.total_tokens}`);
    }
    
    const content = data.choices[0]?.message?.content;
    const finishReason = data.choices[0]?.finish_reason;
    
    console.log(`[LLM] üèÅ Finish Reason: ${finishReason || 'N/A'}`);
    console.log(`[LLM] üìÑ Raw Content: ${content}`);
    
    if (!content) {
      console.log(`[LLM] ‚ùå Empty response content!`);
      throw new Error('Empty response from OpenAI');
    }

    const parsed = JSON.parse(content) as { X?: number; Y?: number };
    
    console.log(`[LLM] üîç Parsed JSON: ${JSON.stringify(parsed)}`);
    
    // Validate response
    if (!validateScore(parsed)) {
      console.log(`[LLM] ‚ùå Invalid score format!`);
      console.log(`[LLM]    X type: ${typeof parsed.X}, value: ${parsed.X}`);
      console.log(`[LLM]    Y type: ${typeof parsed.Y}, value: ${parsed.Y}`);
      throw new Error('Invalid score format');
    }

    console.log('='.repeat(60));
    console.log(`[LLM] ‚úÖ FINAL SCORE: X=${parsed.X}, Y=${parsed.Y}`);
    console.log('='.repeat(60));
    console.log('');
    
    return {
      X: parsed.X!,
      Y: parsed.Y!
    };
  } catch (error) {
    console.log(`[LLM] ‚ùå Error occurred: ${(error as Error).message}`);
    
    // Retry once on failure
    if (retryCount < 1) {
      console.log(`[LLM] üîÑ Retrying (attempt ${retryCount + 2})...`);
      return scoreTask(config, taskName, taskDescription, rubric, retryCount + 1);
    }
    
    console.log(`[LLM] ‚ùå Max retries exceeded. Giving up.`);
    throw error;
  }
}

function validateScore(score: { X?: number; Y?: number }): boolean {
  if (typeof score.X !== 'number' || typeof score.Y !== 'number') {
    return false;
  }
  if (!Number.isInteger(score.X) || !Number.isInteger(score.Y)) {
    return false;
  }
  if (score.X < 1 || score.X > 5 || score.Y < 1 || score.Y > 5) {
    return false;
  }
  return true;
}

export async function scoreTasks(
  config: LLMConfig,
  tasks: Array<{ task_id: string; task_name: string; description?: string }>,
  rubric: Rubric
): Promise<Map<string, ScoreResponse>> {
  const results = new Map<string, ScoreResponse>();
  
  // Process tasks sequentially to avoid rate limiting
  for (const task of tasks) {
    try {
      const score = await scoreTask(config, task.task_name, task.description, rubric);
      results.set(task.task_id, score);
    } catch (error) {
      console.error(`Failed to score task ${task.task_id}:`, error);
      // Mark as needing manual input
      results.set(task.task_id, { X: -1, Y: -1 });
    }
  }
  
  return results;
}
